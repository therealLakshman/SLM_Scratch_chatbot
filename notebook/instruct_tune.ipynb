{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mtEcbXA3mlVK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from datasets import load_dataset, Dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import tiktoken\n",
        "import random\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                       .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash:\n",
        "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "    # Add a get method to mimic dictionary-like access for peft\n",
        "    def get(self, key, default=None):\n",
        "        return getattr(self, key, default)\n",
        "\n",
        "    # Add __contains__ and __getitem__ for dictionary-like behavior for PEFT\n",
        "    def __contains__(self, key):\n",
        "        return hasattr(self, key)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if hasattr(self, key):\n",
        "            return getattr(self, key)\n",
        "        raise KeyError(f\"'{key}' not found in GPTConfig\")\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop=nn.Dropout(config.dropout),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "            return logits, loss\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            return logits, None\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Generate tokens given a conditioning sequence.\n",
        "        idx: Tensor of shape (B, T)\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "A_t-zzGPmrzE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Instruction Dataset"
      ],
      "metadata": {
        "id": "spPXwvkOA-O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import random"
      ],
      "metadata": {
        "id": "TrBX9FdV_SCt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. TinyStories instructions (90% of data)\n",
        "ds_tiny = load_dataset(\"roneneldan/TinyStories\", split=\"train\")"
      ],
      "metadata": {
        "id": "NXytIJ-KFv_p"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "templates = [\n",
        "    \"Tell a short children's story about {topic}.\",\n",
        "    \"Write a bedtime story featuring {topic}.\",\n",
        "    \"Create a simple tale where {topic}.\",\n",
        "    \"Make up a happy story about {topic}.\",\n",
        "    \"Write a 200-word mini-story about {topic}\",\n",
        "    \"Write a soothing, lyrical bedtime story about a {topic} preparing for sleep.\"\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "M4NU9TmYGPgG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tiny_instruction(example):\n",
        "    story = example[\"text\"].strip()\n",
        "    topic = \" \".join(story.split()[:12]).rsplit(\".\", 1)[0] + \".\"\n",
        "    instruction = random.choice(templates).format(topic=topic)\n",
        "    return {\"instruction\": instruction, \"output\": story}\n",
        "\n",
        "tiny_pairs = ds_tiny.shuffle(seed=42).select(range(12000)).map(make_tiny_instruction, num_proc=4)"
      ],
      "metadata": {
        "id": "6vYPHyvTGT1y"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Safe Alpaca stories (10% of data)\n",
        "alpaca = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
        "\n",
        "story_keywords = [\"story\", \"tale\", \"fable\", \"bedtime story\", \"short story\", \"children's story\", \"fairytale\"]\n",
        "def is_child_story(example):\n",
        "    inst = example[\"instruction\"].lower()\n",
        "    output = example[\"output\"]\n",
        "    return (any(k in inst for k in story_keywords) and\n",
        "            len(output.split()) < 400 and\n",
        "            all(bad not in output.lower() for bad in [\"sex\", \"kill\", \"die\", \"blood\", \"erotic\", \"adult\", \"murder\", \"horror\"]))\n",
        "\n",
        "alpaca_stories = alpaca.filter(is_child_story, num_proc=4).shuffle(seed=42).select(range(1300))  # ~10%"
      ],
      "metadata": {
        "id": "j4fAnGVTGilS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_list = list(tiny_pairs) + list(alpaca_stories)\n",
        "final_dataset = Dataset.from_list(final_list).shuffle(seed=42)\n",
        "\n",
        "print(f\"Final instruction-tuning dataset ready: {len(final_dataset)} examples\")\n",
        "print(\"Example:\", final_dataset[5][\"instruction\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vp8uU4qGo-r",
        "outputId": "177eecf4-4550-4072-fff2-ae629440c353"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final instruction-tuning dataset ready: 13300 examples\n",
            "Example: Create a simple tale where Timmy was outside playing all by himself..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== 3. DATASET CLASS ==========================\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "block_size = 128\n",
        "\n",
        "class StoryDataset(TorchDataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, i):\n",
        "        item = self.data[i]\n",
        "        text = f\"Instruction: {item['instruction']}\\n\\nResponse: {item['output']}<|endoftext|>\"\n",
        "        tokens = enc.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "        tokens = tokens[:block_size+1]\n",
        "        if len(tokens) < block_size+1:\n",
        "            tokens += [50256] * (block_size+1 - len(tokens))\n",
        "        x = torch.tensor(tokens[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(tokens[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "train_ds = StoryDataset(final_dataset)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "G1GCBA0fGw2U"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== 4. LOAD MODEL + LoRA ==========================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "config = GPTConfig(\n",
        "    vocab_size=50257,\n",
        "    block_size=128,\n",
        "    n_layer=6,\n",
        "    n_head=6,\n",
        "    n_embd=384,\n",
        "    dropout=0.1,\n",
        "    bias=True\n",
        ")\n",
        "model = GPT(config).to(device)\n",
        "\n",
        "# Load your pretrained weights\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/SLM Model/best_model_params.pt\", map_location=device))\n",
        "\n",
        "# Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16, lora_alpha=32,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    lora_dropout=0.05, bias=\"none\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWLiy0JdIOcZ",
        "outputId": "1605eb56-9af4-4eac-c9d9-1f4c090f40d2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 405,504 || all params: 30,400,896 || trainable%: 1.3339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=3*len(train_loader))\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for step, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, loss = model(x, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1} | Step {step} | Loss {loss.item():.4f}\")\n",
        "    print(f\"Epoch {epoch+1} finished — Avg loss: {total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnNWetRcLExx",
        "outputId": "1e91b9bf-53c4-4725-c408-520b4f793cf8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Step 0 | Loss 3.4053\n",
            "Epoch 1 | Step 100 | Loss 2.4948\n",
            "Epoch 1 | Step 200 | Loss 2.1010\n",
            "Epoch 1 | Step 300 | Loss 1.7999\n",
            "Epoch 1 | Step 400 | Loss 2.0889\n",
            "Epoch 1 finished — Avg loss: 2.2725\n",
            "Epoch 2 | Step 0 | Loss 2.1801\n",
            "Epoch 2 | Step 100 | Loss 2.3217\n",
            "Epoch 2 | Step 200 | Loss 1.8854\n",
            "Epoch 2 | Step 300 | Loss 2.1302\n",
            "Epoch 2 | Step 400 | Loss 2.1854\n",
            "Epoch 2 finished — Avg loss: 2.0016\n",
            "Epoch 3 | Step 0 | Loss 2.1298\n",
            "Epoch 3 | Step 100 | Loss 1.8315\n",
            "Epoch 3 | Step 200 | Loss 1.9146\n",
            "Epoch 3 | Step 300 | Loss 2.0930\n",
            "Epoch 3 | Step 400 | Loss 1.6913\n",
            "Epoch 3 finished — Avg loss: 1.9733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== 6. SAVE ==========================\n",
        "save_dir = \"/content/drive/MyDrive/SLM Model/story_slm_instruct_tuned\"\n",
        "model.save_pretrained(save_dir)\n",
        "print(f\"Instruction-tuned model saved to {save_dir}\")\n",
        "print(\"Done! Now update model_inference.py to load this folder with PeftModel.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWqvM3ezMFwy",
        "outputId": "4474ab2f-1250-43d2-9c74-30ebfd5c71d9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction-tuned model saved to /content/drive/MyDrive/SLM Model/story_slm_instruct_tuned\n",
            "Done! Now update model_inference.py to load this folder with PeftModel.\n"
          ]
        }
      ]
    }
  ]
}